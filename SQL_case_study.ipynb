{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fda26e3-8769-4c99-945a-1fd984bd06ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Initializing Spark**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6c9bcfd-2d99-4ce9-be4b-5fb72d7621f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ebe933b-7ebf-40d6-950e-184c3b9ced2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed209753-38d2-47b4-b726-d03c2988b72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_path = r\"C:\\Users\\sachi\\Downloads\\Exercise\\companies.txt\"\n",
    "invest_path = r\"C:\\Users\\sachi\\Downloads\\Exercise\\InvestmentData.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952b318b-e21e-46a3-ab08-5399e8c942d5",
   "metadata": {},
   "source": [
    "**Reading the data and creating pyspark dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79c91e08-250e-4c24-988c-53e5d70ec9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "investment_df = spark.read.option(\"header\",True).csv(invest_path)\n",
    "companies_df = spark.read.option(\"header\",True).csv(companies_path, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aa463b-cd8e-4654-a563-7c91bd820cad",
   "metadata": {},
   "source": [
    "**Creating views**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3ddbf99-1d90-463a-bbc5-eb7595c6142f",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_df.createOrReplaceTempView(\"companies\")\n",
    "investment_df.createOrReplaceTempView(\"investment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb25cb5-a706-4f9a-85fe-aeda3021c9f4",
   "metadata": {},
   "source": [
    "**Creating a master_df where investment and companies dataset are joined**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c1bdef63-adec-48f4-a79d-96e55b97914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = spark.sql(\"\"\"\n",
    "select \n",
    "company_permalink,\n",
    "funding_round_type,\n",
    "raised_amount_usd,\n",
    "name,\n",
    "split(category_list,\"[|]\")[0] as category_list ,\n",
    "country_code\n",
    " from  investment \n",
    " left join companies\n",
    " on\n",
    "trim(lower(companies.permalink)) = trim(lower(investment.company_permalink))\n",
    "where raised_amount_usd is not null and category_list is not null and country_code is not null\n",
    "\"\"\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddd77d1-70b4-4cde-8190-a3dad6602dbd",
   "metadata": {},
   "source": [
    "**Creating View and caching the master_df since it is used multiple times**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e08d7652-b3e8-4a0b-800b-260b0dd0be10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[company_permalink: string, funding_round_type: string, raised_amount_usd: string, name: string, category_list: string, country_code: string]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df.createOrReplaceTempView(\"master\")\n",
    "master_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7327fd68-80b4-438a-b83e-0d95df27eb35",
   "metadata": {},
   "source": [
    "**Retriving the funding_round_type which has highest number of investment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ef152017-d1f6-4702-af6e-28f033ae5f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|funding_round_type|count|\n",
      "+------------------+-----+\n",
      "|           venture|47809|\n",
      "|              seed|21090|\n",
      "|    debt_financing| 6506|\n",
      "|             angel| 4400|\n",
      "|             grant| 1936|\n",
      "+------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      " Output fetched in  0.3593330383300781 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "#Suitable Funding type \n",
    "spark.sql(\"\"\"\n",
    "select \n",
    "funding_round_type,\n",
    "count(*)  as count\n",
    "from master\n",
    "group by funding_round_type  order by count desc\n",
    "\"\"\"\n",
    ").show(5)\n",
    "print(f\"Output fetched in  {time.time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c711c626-0833-4c4f-9fef-a3cd9faffa5f",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9562b52a-5e70-4c1b-9425-a90960fd7292",
   "metadata": {},
   "source": [
    "**Retriving the top 3 country code for funding_round_type==\"venture\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cb650c6e-2096-4995-8a1b-422b52b081e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+\n",
      "|country_code| invested_amount|\n",
      "+------------+----------------+\n",
      "|         USA|4.20068029342E11|\n",
      "|         CHN| 3.9338918773E10|\n",
      "|         GBR| 2.0072813004E10|\n",
      "|         IND| 1.4261508718E10|\n",
      "|         CAN|   9.482217668E9|\n",
      "+------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Output fetched in  0.5844223499298096 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "spark.sql(\"\"\"\n",
    "select country_code,sum(raised_amount_usd) as invested_amount\n",
    "from master\n",
    "where funding_round_type=\"venture\"\n",
    "group by country_code order by invested_amount desc\n",
    "\"\"\"\n",
    ").show(5)\n",
    "print(f\"Output fetched in  {time.time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23500aca-0369-4d19-a2f1-13d099724f59",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f8dfcc-f153-40c5-ab44-8c1a4faaa0b0",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8073e8d-de18-4bb6-bc2f-481adeb6b614",
   "metadata": {},
   "source": [
    "**Reading the sector data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbf4848-0e16-4be4-988f-b5f30b9b7d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_path = r\"C:\\Users\\sachi\\Downloads\\Exercise\\mapping.csv\"\n",
    "sector_df =  spark.read.option(\"header\",True).csv(sector_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05ba499f-4471-4bd3-90df-b098763ceec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "feb682da-c002-49a9-a456-f7727e255bfe",
   "metadata": {},
   "source": [
    "**Creating Pyspark Melt function to convert column names to rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fab9a8fd-f1ae-447f-ba66-781e18d1e772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types  import *\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from typing import Iterable \n",
    "def melt(\n",
    "        df: DataFrame, \n",
    "        id_vars: Iterable[str], value_vars: Iterable[str], \n",
    "        var_name: str=\"variable\", value_name: str=\"value\") -> DataFrame:\n",
    "    \"\"\"Convert :class:`DataFrame` from wide to long format.\"\"\"\n",
    "\n",
    "    # Create array<struct<variable: str, value: ...>>\n",
    "    _vars_and_vals = array(*(\n",
    "        struct(lit(c).alias(var_name), col(c).alias(value_name)) \n",
    "        for c in value_vars))\n",
    "\n",
    "    # Add to the DataFrame and explode\n",
    "    _tmp = df.withColumn(\"_vars_and_vals\", explode(_vars_and_vals))\n",
    "\n",
    "    cols = id_vars + [\n",
    "            col(\"_vars_and_vals\")[x].alias(x) for x in [var_name, value_name]]\n",
    "    return _tmp.select(*cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8c3e94-f07a-48a5-96a6-927c99ef4287",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa4af48-c79d-45af-a513-d63a42f0b52c",
   "metadata": {},
   "source": [
    "**Melting the sector data and creating view**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d4436108-2540-4b3b-8b01-bf35e2f830ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_df = melt(sector_df,['category_list'],[ i for i in sector_df.columns if i not in 'category_list' ]) \\\n",
    ".filter( (col(\"category_list\").isNotNull()) & (col('value')!=0))\n",
    "sector_df.createOrReplaceTempView(\"sector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590889ac-609b-4acc-8690-b5cd6265a7b4",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fd8b6c-5c9a-4586-ae2b-c82e4f96eda8",
   "metadata": {},
   "source": [
    "**Retriving the top 2 sectors for Country GBR,CHN,USA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "855f12d4-91fd-4183-bc89-e0ce5202150c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------------------------------+------------+----------------+\n",
      "|country_code|main_sector                            |sector_count|sector_sum      |\n",
      "+------------+---------------------------------------+------------+----------------+\n",
      "|GBR         |Others                                 |516         |4.492219646E9   |\n",
      "|GBR         |Cleantech / Semiconductors             |437         |5.052849729E9   |\n",
      "|USA         |Others                                 |8310        |8.2796823598E10 |\n",
      "|USA         |Cleantech / Semiconductors             |7857        |1.18834869645E11|\n",
      "|CHN         |Others                                 |468         |9.26235361E9    |\n",
      "|CHN         |Social, Finance, Analytics, Advertising|281         |9.443388612E9   |\n",
      "+------------+---------------------------------------+------------+----------------+\n",
      "\n",
      "Output fetched in  2.0860347747802734 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "final_df = spark.sql(\"\"\"\n",
    "\n",
    "select country_code,main_sector,sector_count,sector_sum from (\n",
    "select *, row_number() over (partition by country_code order by sector_count desc ) as rank \n",
    "from(\n",
    "select country_code,main_sector,sum(raised_amount_usd) as sector_sum,count(*) as sector_count\n",
    "from\n",
    " master \n",
    "left join \n",
    "(select regexp_replace(category_list,\"0\",\"na\") as category_list \n",
    ",variable as main_sector from sector where variable is not null)sector\n",
    "on \n",
    "master.category_list=sector.category_list\n",
    "where funding_round_type=\"venture\" and country_code in (\"USA\",\"CHN\",\"GBR\")\n",
    "group by 1,2 order by country_code,sector_count\n",
    ")\n",
    ")where rank in (1,2)\n",
    "\"\"\"\n",
    "         )\n",
    "final_df.show(10,False)\n",
    "print(f\"Output fetched in  {time.time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956f349d-8f71-4f29-be1f-33cf610469aa",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94481ba9-e57a-465c-98e3-e03d9f8e5bf9",
   "metadata": {},
   "source": [
    "**We can partition the data based on main_sector. \n",
    "Since the number of sectors are low and limit the number of partitions created at the output.\n",
    "It is also helpful for the faster retrive of data based on sectors for business purpose to understand the investment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d1c398-1ddf-4743-a5dc-12c5f8df2765",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d875fd9e-a483-4967-83cd-a12cfd19aad8",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e0bfae-b8cb-4451-af28-ee47e99699cc",
   "metadata": {},
   "source": [
    "**Explain Plan**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "75f05f4d-95f3-493a-8f62-44fa78478f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(7) Project [country_code#49, main_sector#8165, sector_count#8167L, sector_sum#8166]\n",
      "+- *(7) Filter rank#8168 IN (1,2)\n",
      "   +- Window [row_number() windowspecdefinition(country_code#49, sector_count#8167L DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#8168], [country_code#49], [sector_count#8167L DESC NULLS LAST]\n",
      "      +- *(6) Sort [country_code#49 ASC NULLS FIRST, sector_count#8167L DESC NULLS LAST], false, 0\n",
      "         +- Exchange hashpartitioning(country_code#49, 200), ENSURE_REQUIREMENTS, [id=#4672]\n",
      "            +- *(5) Sort [country_code#49 ASC NULLS FIRST, sector_count#8167L ASC NULLS FIRST], true, 0\n",
      "               +- Exchange rangepartitioning(country_code#49 ASC NULLS FIRST, sector_count#8167L ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#4668]\n",
      "                  +- *(4) HashAggregate(keys=[country_code#49, main_sector#8165], functions=[sum(cast(raised_amount_usd#21 as double)), count(1)])\n",
      "                     +- Exchange hashpartitioning(country_code#49, main_sector#8165, 200), ENSURE_REQUIREMENTS, [id=#4664]\n",
      "                        +- *(3) HashAggregate(keys=[country_code#49, main_sector#8165], functions=[partial_sum(cast(raised_amount_usd#21 as double)), partial_count(1)])\n",
      "                           +- *(3) Project [raised_amount_usd#21, country_code#49, main_sector#8165]\n",
      "                              +- *(3) BroadcastHashJoin [category_list#7416], [category_list#8164], LeftOuter, BuildRight, false\n",
      "                                 :- *(3) Project [raised_amount_usd#21, category_list#7416, country_code#49]\n",
      "                                 :  +- *(3) Filter ((isnotnull(funding_round_type#18) AND (funding_round_type#18 = venture)) AND country_code#49 IN (USA,CHN,GBR))\n",
      "                                 :     +- InMemoryTableScan [category_list#7416, country_code#49, funding_round_type#18, raised_amount_usd#21], [isnotnull(funding_round_type#18), (funding_round_type#18 = venture), country_code#49 IN (USA,CHN,GBR)]\n",
      "                                 :           +- InMemoryRelation [company_permalink#16, funding_round_type#18, raised_amount_usd#21, name#45, category_list#7416, country_code#49], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                                 :                 +- *(2) Project [company_permalink#16, funding_round_type#18, raised_amount_usd#21, name#45, split(category_list#47, [|], -1)[0] AS category_list#71, country_code#49]\n",
      "                                 :                    +- *(2) BroadcastHashJoin [trim(lower(company_permalink#16), None)], [trim(lower(permalink#44), None)], Inner, BuildRight, false\n",
      "                                 :                       :- *(2) Filter isnotnull(raised_amount_usd#21)\n",
      "                                 :                       :  +- FileScan csv [company_permalink#16,funding_round_type#18,raised_amount_usd#21] Batched: false, DataFilters: [isnotnull(raised_amount_usd#21)], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/sachi/Downloads/Exercise/InvestmentData.csv], PartitionFilters: [], PushedFilters: [IsNotNull(raised_amount_usd)], ReadSchema: struct<company_permalink:string,funding_round_type:string,raised_amount_usd:string>\n",
      "                                 :                       +- BroadcastExchange HashedRelationBroadcastMode(List(trim(lower(input[0, string, true]), None)),false), [id=#127]\n",
      "                                 :                          +- *(1) Filter (isnotnull(category_list#47) AND isnotnull(country_code#49))\n",
      "                                 :                             +- FileScan csv [permalink#44,name#45,category_list#47,country_code#49] Batched: false, DataFilters: [isnotnull(category_list#47), isnotnull(country_code#49)], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/sachi/Downloads/Exercise/companies.txt], PartitionFilters: [], PushedFilters: [IsNotNull(category_list), IsNotNull(country_code)], ReadSchema: struct<permalink:string,name:string,category_list:string,country_code:string>\n",
      "                                 +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#4658]\n",
      "                                    +- *(2) Project [regexp_replace(category_list#434, 0, na, 1) AS category_list#8164, _vars_and_vals#701.variable AS main_sector#8165]\n",
      "                                       +- *(2) Filter NOT (cast(_vars_and_vals#701.value as int) = 0)\n",
      "                                          +- Generate explode(array(struct(variable, Automotive & Sports, value, Automotive & Sports#435), struct(variable, Blanks, value, Blanks#436), struct(variable, Cleantech / Semiconductors, value, Cleantech / Semiconductors#437), struct(variable, Entertainment, value, Entertainment#438), struct(variable, Health, value, Health#439), struct(variable, Manufacturing, value, Manufacturing#440), struct(variable, News, Search and Messaging, value, News, Search and Messaging#441), struct(variable, Others, value, Others#442), struct(variable, Social, Finance, Analytics, Advertising, value, Social, Finance, Analytics, Advertising#443))), [category_list#434], false, [_vars_and_vals#701]\n",
      "                                             +- *(1) Filter (isnotnull(category_list#434) AND isnotnull(regexp_replace(category_list#434, 0, na, 1)))\n",
      "                                                +- FileScan csv [category_list#434,Automotive & Sports#435,Blanks#436,Cleantech / Semiconductors#437,Entertainment#438,Health#439,Manufacturing#440,News, Search and Messaging#441,Others#442,Social, Finance, Analytics, Advertising#443] Batched: false, DataFilters: [isnotnull(category_list#434), isnotnull(regexp_replace(category_list#434, 0, na, 1))], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/sachi/Downloads/Exercise/mapping.csv], PartitionFilters: [], PushedFilters: [IsNotNull(category_list)], ReadSchema: struct<category_list:string,Automotive & Sports:string,Blanks:string,Cleantech / Semiconductors:s...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.explain(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b6e4b8-bb7a-46f3-8dd3-cd749bf77946",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
